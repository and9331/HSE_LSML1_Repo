{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BARANOV ANDREY. MDS Spring 2023 Cohort SGA . User routes on the site\n",
    "## Description\n",
    "**Clickstream** is a sequence of user actions on a website. It allows you to understand how users interact with the site. In this task, you need to find the most frequent custom routes.\n",
    "\n",
    "## Input data\n",
    "Input data is а table with clickstream data in file `hdfs:/data/clickstream.csv`.\n",
    "\n",
    "### Table structure\n",
    "* `user_id (int)` - Unique user identifier.\n",
    "* `session_id (int)` - Unique identifier for the user session. The user's session lasts until the identifier changes.\n",
    "* `event_type (string)` - Event type from the list:\n",
    "    * **page** - visit to the page\n",
    "    * **event** - any action on the page\n",
    "    * <b>&lt;custom&gt;</b> - string with any other type\n",
    "* `event_type (string)` - Page on the site.\n",
    "* `timestamp (int)` - Unix-timestamp of action.\n",
    "\n",
    "### Browser errors\n",
    "Errors can sometimes occur in the user's browser - after such an error appears, we can no longer trust the data of this session and all the following lines after the error or at the same time with it are considered corrupted and **should not be counted** in statistics.\n",
    "\n",
    "When an error occurs on the page, a random string containing the word **error** will be written to the `event_type` field.\n",
    "\n",
    "### Sample of user session\n",
    "<pre>\n",
    "+-------+----------+------------+----------+----------+\n",
    "|user_id|session_id|  event_type|event_page| timestamp|\n",
    "+-------+----------+------------+----------+----------+\n",
    "|    562|       507|        page|      main|1620494781|\n",
    "|    562|       507|       event|      main|1620494788|\n",
    "|    562|       507|       event|      main|1620494798|\n",
    "|    562|       507|        page|    family|1620494820|\n",
    "|    562|       507|       event|    family|1620494828|\n",
    "|    562|       507|        page|      main|1620494848|\n",
    "|    562|       507|wNaxLlerrorU|      main|1620494865|\n",
    "|    562|       507|       event|      main|1620494873|\n",
    "|    562|       507|        page|      news|1620494875|\n",
    "|    562|       507|        page|   tariffs|1620494876|\n",
    "|    562|       507|       event|   tariffs|1620494884|\n",
    "|    562|       514|        page|      main|1620728918|\n",
    "|    562|       514|       event|      main|1620729174|\n",
    "|    562|       514|        page|   archive|1620729674|\n",
    "|    562|       514|        page|     bonus|1620729797|\n",
    "|    562|       514|        page|   tariffs|1620731090|\n",
    "|    562|       514|       event|   tariffs|1620731187|\n",
    "+-------+----------+------------+----------+----------+\n",
    "</pre>\n",
    "\n",
    "#### Correct user routes for a given user:\n",
    "* **Session 507**: main-family-main\n",
    "* **Session 514**: main-archive-bonus-tariffs\n",
    "\n",
    "Route elements are ordered by the time they appear in the clickstream, from earliest to latest.\n",
    "\n",
    "The route must be accounted for completely before the end of the session or an error in the session.\n",
    "\n",
    "## Task\n",
    "You need to use the Spark SQL, Spark RDD and Spark DF interfaces to create a solution file, the lines of which contain **the 30 most frequent user routes** on the site.\n",
    "\n",
    "Each line of the file should contain the `route` and `count` values **separated by tabs**, where:\n",
    "* `route` - route on the site, consisting of pages separated by \"-\".\n",
    "* `count` - the number of user sessions in which this route was.\n",
    "\n",
    "The lines must be **ordered in descending order** of the `count` field.\n",
    "\n",
    "## Criteria\n",
    "You can get maximum of 3.5 points (final grade) for this assignment, depedning on the number of interface you manage to leverage. The criteria are as follows:\n",
    "\n",
    "* 0.5 points – Spark SQL solution with 1 query\n",
    "* 0.5 points – Spark SQL solution with <=2 queries\n",
    "* 0.5 points – Spark RDD solution\n",
    "* 0.5 points – Spark DF solution\n",
    "* 0.5 points – your solution algorithm is relatively optimized, i.e.: no O^2 or O^3 complexities; appropriate object usage; no data leaks etc. This is evaluated by staff.\n",
    "* 1 point – 1 on 1 screening session. During this session staff member can ask you questions regarding your solution logic, framework usage, questionable parts of your code etc. If your code is clean enough, the staff member can just ask you to solve a theoretical problem connected to Spark.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Spark DF solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# setting up environment\n",
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# spark imports\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession, Row\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.types import StringType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "2024-10-27 18:39:17,187 WARN yarn.Client: Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to uploading libraries under SPARK_HOME.\n"
     ]
    }
   ],
   "source": [
    "# Initialize SparkContext and SparkSession\n",
    "sc = pyspark.SparkContext(appName='jupyter')\n",
    "spark = SparkSession(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# loading the CSV file into a DataFrame\n",
    "clickstream_df = spark.read.csv('/data/clickstream.csv', header=True, sep = '\\t', inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- user_id: integer (nullable = true)\n",
      " |-- session_id: integer (nullable = true)\n",
      " |-- event_type: string (nullable = true)\n",
      " |-- event_page: string (nullable = true)\n",
      " |-- timestamp: integer (nullable = true)\n",
      "\n",
      "+-------+----------+------------+----------+----------+\n",
      "|user_id|session_id|  event_type|event_page| timestamp|\n",
      "+-------+----------+------------+----------+----------+\n",
      "|    562|       507|        page|      main|1695584127|\n",
      "|    562|       507|       event|      main|1695584134|\n",
      "|    562|       507|       event|      main|1695584144|\n",
      "|    562|       507|       event|      main|1695584147|\n",
      "|    562|       507|wNaxLlerrorU|      main|1695584154|\n",
      "+-------+----------+------------+----------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# showing the data structure to understand columns and data types\n",
    "clickstream_df.printSchema()\n",
    "clickstream_df.show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create a window specification for each session, ordered by the timestamp\n",
    "w_spec = Window.partitionBy(\"user_id\", \"session_id\").orderBy(\"timestamp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Determine the timestamp of the first error occurrence within each session\n",
    "clickstream_df_with_errors = clickstream_df.withColumn(\"error_timestamp\", F.min(F.when(F.col(\"event_type\").like(\"%error%\"), F.col(\"timestamp\"))).over(w_spec))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Exclude rows that occur at or after the identified error timestamp\n",
    "clickstream_filtered = clickstream_df_with_errors.filter((F.col(\"error_timestamp\").isNull()) | (F.col(\"timestamp\") < F.col(\"error_timestamp\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Order events within each session by user ID, session ID, and timestamp\n",
    "clickstream_sorted = clickstream_filtered.orderBy(\"user_id\", \"session_id\", \"timestamp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# User Defined Function (UDF) to eliminate consecutive duplicates\n",
    "def eliminate_consecutive_duplicates(event_pages):\n",
    "    if not event_pages:\n",
    "        return \"\"\n",
    "    deduplicated_pages = [event_pages[0]]\n",
    "    for page in event_pages[1:]:\n",
    "        if page != deduplicated_pages[-1]:\n",
    "            deduplicated_pages.append(page)\n",
    "    return '-'.join(deduplicated_pages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "eliminate_consecutive_udf = F.udf(eliminate_consecutive_duplicates, StringType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Aggregate by session, collect event pages, remove consecutive duplicates, and generate routes\n",
    "clickstream_routes = clickstream_sorted.groupBy(\"user_id\", \"session_id\").agg(eliminate_consecutive_udf(F.collect_list(\"event_page\")).alias(\"route\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Count each unique route's occurrences and sort the results in descending order\n",
    "clickstream_routes_counts = clickstream_routes.groupBy(\"route\").count().orderBy(F.desc(\"count\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Retrieve the top 30 most frequent routes\n",
    "result = clickstream_routes_counts.limit(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 14:===================>                                      (2 + 2) / 6]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n",
      "|               route|count|\n",
      "+--------------------+-----+\n",
      "|                main| 8184|\n",
      "|        main-archive| 1113|\n",
      "|         main-rabota| 1047|\n",
      "|       main-internet|  897|\n",
      "|          main-bonus|  870|\n",
      "|           main-news|  769|\n",
      "|        main-tariffs|  677|\n",
      "|         main-online|  587|\n",
      "|          main-vklad|  518|\n",
      "| main-rabota-archive|  170|\n",
      "| main-archive-rabota|  167|\n",
      "|  main-bonus-archive|  143|\n",
      "|   main-rabota-bonus|  138|\n",
      "|   main-bonus-rabota|  135|\n",
      "|    main-news-rabota|  135|\n",
      "|main-archive-inte...|  132|\n",
      "|    main-rabota-news|  130|\n",
      "|main-internet-rabota|  129|\n",
      "|   main-archive-news|  126|\n",
      "|main-rabota-internet|  124|\n",
      "+--------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Show the final results\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Collect results to a list of tuples\n",
    "result_list = result.collect()\n",
    "\n",
    "# Format the results as a dictionary for JSON\n",
    "results_dict = {route: count for route, count in result_list}\n",
    "\n",
    "# Save the results to a JSON file\n",
    "with open('result.json', 'w') as json_file:\n",
    "    json.dump(results_dict, json_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9999999999999999\n",
      "Correct main answer!\n",
      "Correct main-archive answer!\n",
      "Correct main-rabota answer!\n",
      "Correct main-internet answer!\n",
      "Correct main-bonus answer!\n",
      "Correct main-news answer!\n",
      "Correct main-tariffs answer!\n",
      "Correct main-online answer!\n",
      "Correct main-vklad answer!\n",
      "Correct main-rabota-archive answer!\n",
      "Wrong main-archive-rabota answer!\n",
      "Wrong main-bonus-archive answer!\n",
      "Wrong main-rabota-bonus answer!\n",
      "Wrong main-news-rabota answer!\n",
      "Wrong main-bonus-rabota answer!\n",
      "Wrong main-archive-internet answer!\n",
      "Wrong main-rabota-news answer!\n",
      "Wrong main-internet-rabota answer!\n",
      "Wrong main-archive-news answer!\n",
      "Wrong main-rabota-internet answer!\n",
      "Wrong main-internet-archive answer!\n",
      "Wrong main-archive-bonus answer!\n",
      "Wrong main-internet-bonus answer!\n",
      "Wrong main-tariffs-internet answer!\n",
      "Wrong main-news-archive answer!\n",
      "Wrong main-news-internet answer!\n",
      "Wrong main-archive-tariffs answer!\n",
      "Wrong main-internet-news answer!\n",
      "Wrong main-tariffs-archive answer!\n",
      "Wrong main-rabota-main answer!\n"
     ]
    }
   ],
   "source": [
    "#!curl -F file=@result.json 51.250.123.136:80/MDS-LSML1/nndrew/w6/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark SQL solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-27 18:46:06,667 WARN yarn.Client: Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to uploading libraries under SPARK_HOME.\n",
      "[Stage 13:========================>                                 (3 + 2) / 7]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n",
      "|               route|count|\n",
      "+--------------------+-----+\n",
      "|                main| 8184|\n",
      "|        main-archive| 1113|\n",
      "|         main-rabota| 1047|\n",
      "|       main-internet|  897|\n",
      "|          main-bonus|  870|\n",
      "|           main-news|  769|\n",
      "|        main-tariffs|  677|\n",
      "|         main-online|  587|\n",
      "|          main-vklad|  518|\n",
      "| main-rabota-archive|  170|\n",
      "| main-archive-rabota|  167|\n",
      "|  main-bonus-archive|  143|\n",
      "|   main-rabota-bonus|  139|\n",
      "|   main-bonus-rabota|  135|\n",
      "|    main-news-rabota|  135|\n",
      "|main-archive-inte...|  132|\n",
      "|    main-rabota-news|  130|\n",
      "|main-internet-rabota|  129|\n",
      "|   main-archive-news|  126|\n",
      "|main-rabota-internet|  124|\n",
      "+--------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Setting up environment\n",
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "# Spark imports\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import StringType\n",
    "\n",
    "# Initialize SparkContext and SparkSession\n",
    "sc = pyspark.SparkContext(appName='jupyter')\n",
    "spark = SparkSession(sc)\n",
    "\n",
    "# Load the CSV file into a DataFrame\n",
    "clickstream_df = spark.read.csv('/data/clickstream.csv', header=True, sep='\\t', inferSchema=True)\n",
    "\n",
    "# Create a temporary view for SQL queries\n",
    "clickstream_df.createOrReplaceTempView(\"clickstream\")\n",
    "\n",
    "# Determine the timestamp of the first error occurrence within each session\n",
    "error_timestamp_query = \"\"\"\n",
    "SELECT user_id,\n",
    "       session_id,\n",
    "       timestamp,\n",
    "       event_type,\n",
    "       event_page,\n",
    "       MIN(CASE WHEN event_type LIKE '%error%' THEN timestamp END) OVER (PARTITION BY user_id, session_id) AS error_timestamp\n",
    "FROM clickstream\n",
    "\"\"\"\n",
    "\n",
    "error_timestamp_df = spark.sql(error_timestamp_query)\n",
    "error_timestamp_df.createOrReplaceTempView(\"clickstream_with_errors\")\n",
    "\n",
    "# Exclude rows that occur at or after the identified error timestamp\n",
    "filtered_query = \"\"\"\n",
    "SELECT user_id,\n",
    "       session_id,\n",
    "       timestamp,\n",
    "       event_type,\n",
    "       event_page\n",
    "FROM clickstream_with_errors\n",
    "WHERE error_timestamp IS NULL OR timestamp < error_timestamp\n",
    "ORDER BY user_id, session_id, timestamp\n",
    "\"\"\"\n",
    "\n",
    "filtered_df = spark.sql(filtered_query)\n",
    "filtered_df.createOrReplaceTempView(\"filtered_clickstream\")\n",
    "\n",
    "# Use SQL to eliminate consecutive duplicates and create routes\n",
    "route_query = \"\"\"\n",
    "WITH ranked_pages AS (\n",
    "    SELECT user_id,\n",
    "           session_id,\n",
    "           event_page,\n",
    "           ROW_NUMBER() OVER (PARTITION BY user_id, session_id ORDER BY timestamp) AS rn,\n",
    "           LAG(event_page) OVER (PARTITION BY user_id, session_id ORDER BY timestamp) AS prev_page\n",
    "    FROM filtered_clickstream\n",
    ")\n",
    "SELECT user_id,\n",
    "       session_id,\n",
    "       COLLECT_LIST(event_page) AS pages\n",
    "FROM ranked_pages\n",
    "WHERE event_page != prev_page OR prev_page IS NULL\n",
    "GROUP BY user_id, session_id\n",
    "\"\"\"\n",
    "\n",
    "routes_df = spark.sql(route_query)\n",
    "routes_df.createOrReplaceTempView(\"routes\")\n",
    "\n",
    "# Generate the final routes and count occurrences\n",
    "count_routes_query = \"\"\"\n",
    "SELECT CONCAT_WS('-', pages) AS route,\n",
    "       COUNT(*) AS count\n",
    "FROM routes\n",
    "GROUP BY route\n",
    "ORDER BY count DESC\n",
    "LIMIT 30\n",
    "\"\"\"\n",
    "\n",
    "result = spark.sql(count_routes_query)\n",
    "\n",
    "# Show the final results\n",
    "result.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Collect results to a list of tuples\n",
    "result_list = result.collect()\n",
    "\n",
    "# Format the results as a dictionary for JSON\n",
    "results_dict = {route: count for route, count in result_list}\n",
    "\n",
    "# Save the results to a JSON file\n",
    "with open('result.json', 'w') as json_file:\n",
    "    json.dump(results_dict, json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#!curl -F file=@result.json 51.250.123.136:80/MDS-LSML1/nndrew/w6/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Spark RDD solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Setting up environment\n",
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "# Spark imports\n",
    "import pyspark\n",
    "from pyspark import SparkContext\n",
    "\n",
    "# Initialize SparkContext\n",
    "sc = SparkContext(appName='jupyter')\n",
    "\n",
    "# Load the CSV file into an RDD\n",
    "clickstream_rdd = sc.textFile('/data/clickstream.csv')\n",
    "\n",
    "# Parse the CSV data and create an RDD of tuples\n",
    "header = clickstream_rdd.first()  # Get the header\n",
    "clickstream_rdd = clickstream_rdd.filter(lambda line: line != header)  # Remove header\n",
    "\n",
    "# Split the lines into fields\n",
    "clickstream_rdd = clickstream_rdd.map(lambda line: line.split('\\t'))\n",
    "\n",
    "# Define a function to filter out error sessions and collect valid events\n",
    "def process_session(events):\n",
    "    valid_events = []\n",
    "    error_found = False\n",
    "    for event in events:\n",
    "        user_id = int(event[0])\n",
    "        session_id = int(event[1])\n",
    "        event_type = event[2]\n",
    "        event_page = event[3]\n",
    "        timestamp = int(event[4])\n",
    "        \n",
    "        # Check for error event\n",
    "        if 'error' in event_type:\n",
    "            error_found = True\n",
    "            break\n",
    "        \n",
    "        if not error_found:\n",
    "            if valid_events and valid_events[-1][3] == event_page:  # Avoid duplicates\n",
    "                continue\n",
    "            valid_events.append((user_id, session_id, event_page, timestamp))\n",
    "    \n",
    "    return valid_events\n",
    "\n",
    "# Group events by (user_id, session_id) and process each session\n",
    "grouped_sessions = clickstream_rdd.map(lambda event: ((int(event[0]), int(event[1])), (event[3], int(event[4])))) \\\n",
    "    .groupByKey() \\\n",
    "    .mapValues(list)  # Convert to list for processing\n",
    "\n",
    "# Process each session to filter errors and eliminate consecutive duplicates\n",
    "valid_routes = grouped_sessions.flatMap(lambda x: process_session(x[1]))\n",
    "\n",
    "# Create routes by session\n",
    "session_routes = valid_routes.map(lambda x: ((x[0], x[1]), x[2]))  # (user_id, session_id) -> event_page\n",
    "session_routes = session_routes.groupByKey()  # Group by user_id and session_id\n",
    "\n",
    "# Build the final routes and count occurrences\n",
    "final_routes = session_routes.map(lambda x: ('-'.join(x[1]), 1))  # Create route string\n",
    "route_counts = final_routes.reduceByKey(lambda a, b: a + b)  # Count occurrences of each route\n",
    "\n",
    "# Sort and get the top 30 routes\n",
    "top_routes = route_counts.takeOrdered(30, key=lambda x: -x[1])  # Sort by count descending\n",
    "\n",
    "# Show the final results\n",
    "for route, count in top_routes:\n",
    "    print(f\"{route}\\t{count}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "coursera": {
   "schema_names": [
    "week-4-spark-homework"
   ]
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
